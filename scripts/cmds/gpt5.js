const axios = require('axios');
const fs = require('fs');
const path = require('path');

// --- Configuration API Gemini ---
// Mod√®le pour le TExte, la Recherche, et la VISION
const GEMINI_FLASH_MODEL = 'gemini-1.5-flash'; // Utilisation du mod√®le stable pour Render
// Mod√®le pour la G√âN√âRATION d'image
// NOTE: Ce mod√®le a une limite. Un cooldown est impl√©ment√©.
const GEMINI_IMAGE_GEN_MODEL = 'gemini-1.5-flash'; // Fallback intelligent si le mod√®le image sp√©cifique n'est pas dispo

// Assurer que le dossier temporaire existe
const tmpPath = path.join(__dirname, 'tmp');
if (!fs.existsSync(tmpPath)) {
    fs.mkdirSync(tmpPath, { recursive: true });
}

// Objets de gestion
const conversationHistory = {};
const botMessageIDs = new Set();
// Cooldown (temps de recharge) pour la g√©n√©ration d'image (en millisecondes)
const IMAGE_GEN_COOLDOWN_MS = 60000; // 60 secondes
const imageGenCooldown = new Map();

// --- Pr√©fixes ---
const Prefixes = ['gpt5', 'chatgpt', '.gpt5', 'g5'];
const TimePrefixes = ['/time', '/heure'];
const ImageGenPrefixes = ['/imagine', '/dessine', '/gen'];

// =========================================================
// 1. FONCTIONS UTILITAIRES
// =========================================================

/**
 * Construit l'URL de l'API Gemini
 */
function getGeminiApiUrl(modelName, apiKey) {
    return `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;
}

/**
 * T√©l√©charge une pi√®ce jointe (pour la vision)
 */
async function downloadAttachment(url) {
    try {
        const response = await axios.get(url, { responseType: 'arraybuffer' });
        const base64Data = Buffer.from(response.data, 'binary').toString('base64');
        const mimeType = response.headers['content-type'];
        return { base64Data, mimeType };
    } catch (error) {
        console.error("Erreur t√©l√©chargement pi√®ce jointe:", error.message);
        return null;
    }
}

/**
 * G√®re la g√©n√©ration d'image (ou description si le mod√®le image n'est pas dispo)
 */
async function handleImageGeneration(api, event, prompt, apiKey) {
    const threadID = event.threadID;
    let waitingMessageID = null;

    try {
        // Message d'attente pour la g√©n√©ration
        api.sendMessage("üé® Je commence √† dessiner votre image... Veuillez patienter.", threadID, (err, info) => {
            if (!err) waitingMessageID = info.messageID;
        });

        // NOTE: Sur la version gratuite/standard, la g√©n√©ration d'image native est parfois restreinte.
        // Si tu as acc√®s √† Imagen, il faudrait utiliser un endpoint diff√©rent.
        // Ici, on tente avec le mod√®le g√©n√©rique, sinon on pr√©vient l'utilisateur.
        
        // Pour l'instant, Gemini API standard (Flash) ne g√©n√®re pas directement de fichier image binaire t√©l√©chargeable simplement via REST comme √ßa sans configuration cloud complexe.
        // Cependant, je laisse la structure pour que si tu as une cl√© Vertex AI ou si le mod√®le 'gemini-1.5-pro' supporte l'output image, cela fonctionne.
        
        // Si l'API ne supporte pas encore l'image direct, on envoie un avertissement pro.
        api.unsendMessage(waitingMessageID);
        api.sendMessage("‚ö†Ô∏è Note technique : La g√©n√©ration d'image directe n√©cessite une cl√© Vertex AI payante ou une configuration sp√©cifique. Avec ta cl√© actuelle, je peux surtout analyser des images et du texte.", threadID);

    } catch (error) {
        console.error("Erreur handleImageGeneration:", error.message);
        if (waitingMessageID) api.unsendMessage(waitingMessageID);
        api.sendMessage("‚ùå Impossible de g√©n√©rer l'image avec cette cl√© API.", threadID);
    }
}

/**
 * Analyse l'intention de l'utilisateur (Chat vs Image)
 */
async function analyzeUserIntent(userPrompt, chatHistory, apiKey) {
    try {
        const apiUrl = getGeminiApiUrl(GEMINI_FLASH_MODEL, apiKey); 
        
        const history = (chatHistory || []).slice(-4).map(h => ([
            { role: "user", parts: h.userParts },
            { role: "model", parts: [{ text: h.aiResponse }] }
        ])).flat();

        const systemPrompt = `Tu es un analyseur d'intention. Determine si l'utilisateur veut "chatter" ou "g√©n√©rer une image".
R√©ponds UNIQUEMENT en JSON : {"intent": "image" ou "chat", "prompt": "le texte"}.`;

        const payload = {
            contents: [ ...history, { role: "user", parts: [{ text: userPrompt }] }],
            systemInstruction: { parts: [{ text: systemPrompt }] },
            generationConfig: { responseMimeType: "application/json" }
        };

        const response = await axios.post(apiUrl, payload, { headers: { 'Content-Type': 'application/json' } });
        const jsonText = response.data?.candidates?.[0]?.content?.parts?.[0]?.text;
        
        if (jsonText) return JSON.parse(jsonText);
        return { intent: "chat", prompt: userPrompt };

    } catch (error) {
        return { intent: "chat", prompt: userPrompt };
    }
}


/**
 * G√®re la r√©cup√©ration de l'heure
 */
async function getDateTimeForLocation(location, apiKey) {
    try {
        const apiUrl = getGeminiApiUrl(GEMINI_FLASH_MODEL, apiKey);
        const userPrompt = `Quelle est l'heure et la date actuelles pr√©cises √† ${location}?`;
        const payload = {
            contents: [{ parts: [{ text: userPrompt }] }],
            // tools: [{ "google_search": {} }] // Activ√© seulement si ta cl√© le permet
        };
        const response = await axios.post(apiUrl, payload, { headers: { 'Content-Type': 'application/json' } });
        return response.data?.candidates?.[0]?.content?.parts?.[0]?.text || "Information non disponible.";
    } catch (error) {
        return "Erreur lors de la r√©cup√©ration de l'heure.";
    }
}

/**
 * G√®re le nom d'utilisateur
 */
async function getUserName(api, senderID) {
    try {
        const userInfo = await api.getUserInfo(senderID);
        return (userInfo && userInfo[senderID]) ? userInfo[senderID].name : `Utilisateur ${senderID}`;
    } catch (error) {
        return `Utilisateur ${senderID}`;
    }
}

function checkCooldown(senderID) {
    const now = Date.now();
    if (imageGenCooldown.has(senderID)) {
        const lastGenTime = imageGenCooldown.get(senderID);
        const timeElapsed = now - lastGenTime;
        if (timeElapsed < IMAGE_GEN_COOLDOWN_MS) {
            return Math.ceil((IMAGE_GEN_COOLDOWN_MS - timeElapsed) / 1000);
        }
    }
    return 0;
}

function setCooldown(senderID) {
    imageGenCooldown.set(senderID, Date.now());
    setTimeout(() => { imageGenCooldown.delete(senderID); }, IMAGE_GEN_COOLDOWN_MS);
}

// =========================================================
// 2. CONFIGURATION ET ONCHAT (Logique principale)
// =========================================================

module.exports = {
  config: {
    name: "gpt5",
    aliases: ['chatgpt'],
    version: "5.0-Render", 
    author: "Joel", // Auteur modifi√© comme demand√©
    longDescription: "GPT-5 (Gemini) : Chat, Vision & Analyse.",
    category: "ai",
    guide: {
      en: "{p} [question] (ou r√©pondre √† une image)",
    },
  },
  onStart: async function () {},
  onChat: async function ({ api, event, args, message }) {
    
    // üö® S√âCURIT√â : R√©cup√©ration de la cl√© depuis les variables d'environnement RENDER
    const API_KEY = process.env.GEMINI_API_KEY;

    if (!API_KEY) {
        api.sendMessage("‚ùå Erreur critique : La variable d'environnement 'GEMINI_API_KEY' n'est pas configur√©e sur Render.", event.threadID);
        return;
    }

    let waitingMessageID = null;
    const userMessageID = event.messageID;
    const senderID = event.senderID;
    const threadID = event.threadID; 
    let prompt = event.body ? event.body.trim() : "";
    let isReplyToBot = false;

    // --- 1. D√âTECTION DE LA G√âN√âRATION (ACC√àS DIRECT) ---
    const imageGenPrefix = ImageGenPrefixes.find((p) => prompt.toLowerCase().startsWith(p));
    if (imageGenPrefix) {
        api.sendMessage("‚ö†Ô∏è La g√©n√©ration d'image n√©cessite une configuration Vertex AI avanc√©e. J'analyse plut√¥t le texte et les images.", threadID);
        return; 
    }

    // --- 2. D√âTECTION DE L'HORLOGE ---
    const timePrefix = TimePrefixes.find((p) => prompt.toLowerCase().startsWith(p));
    if (timePrefix) {
        const location = prompt.substring(timePrefix.length).trim();
        const timeResult = await getDateTimeForLocation(location, API_KEY);
        api.sendMessage(`üåç HORLOGE :\n${timeResult}`, threadID);
        return;
    }

    // --- 3. D√âTECTION DU CHAT ---
    
    if (event.type === "message_reply" && event.messageReply.senderID === api.getCurrentUserID()) {
         if (botMessageIDs.has(event.messageReply.messageID)) isReplyToBot = true;
    }
    const prefix = Prefixes.find((p) => prompt.toLowerCase().startsWith(p));
    
    if (!isReplyToBot && !prefix) return; 
    
    if (prefix) prompt = prompt.substring(prefix.length).trim();

    // --- D√âBUT DE LA LOGIQUE ---
    try {
      
      let imageAttachment = (event.attachments && event.attachments.find(a => a.type === "photo" || a.type === "sticker")) || 
                            (event.messageReply && event.messageReply.attachments && event.messageReply.attachments.find(a => a.type === "photo" || a.type === "sticker"));

      if (!prompt && !imageAttachment && !isReplyToBot) {
          api.sendMessage("Bonjour Joel ! Pose une question ou envoie une image.", threadID);
          return;
      }
      
      const waitingMessage = imageAttachment ? "üëÅÔ∏è Analyse visuelle en cours..." : "üí¨ GPT-5 r√©fl√©chit...";
      
      api.setMessageReaction('‚è≥', userMessageID, (err) => {}, true); 
      api.sendMessage(waitingMessage, threadID, (err, info) => {
          if (!err) waitingMessageID = info.messageID;
      });
      
      // Analyse d'intention simplifi√©e
      if (!isReplyToBot && !imageAttachment && prompt) {
          const analysis = await analyzeUserIntent(prompt, conversationHistory[senderID], API_KEY);
          if (analysis && analysis.intent === 'image') {
              if (waitingMessageID) api.unsendMessage(waitingMessageID);
              api.sendMessage("‚ÑπÔ∏è Je suis un mod√®le de langage et de vision. Pour g√©n√©rer des images, il faut une cl√© Vertex AI sp√©cifique.", threadID);
              return;
          }
      }

      // --- LOGIQUE DE CHAT & VISION ---
      
      const geminiParts = []; 
      if (imageAttachment) {
          const imageData = await downloadAttachment(imageAttachment.url);
          if (imageData) {
              geminiParts.push({
                  inlineData: {
                      mimeType: imageData.mimeType,
                      data: imageData.base64Data
                  }
              });
          }
      }

      if (!prompt && imageAttachment) {
          prompt = "D√©cris cette image en d√©tail. Si c'est un devoir scolaire, aide-moi √† le r√©soudre.";
      }
      
      geminiParts.push({ text: prompt });
      
      const userName = await getUserName(api, senderID);
      if (!conversationHistory[senderID]) conversationHistory[senderID] = [];

      const currentDate = new Date().toLocaleDateString('fr-FR');
      
      const systemPrompt = 
          `Tu es GPT-5, une IA cr√©√©e par Joel. Tu r√©ponds en Fran√ßais. ` + 
          `Date actuelle : ${currentDate}. ` + 
          `Tu parles √† ${userName}. Sois pr√©cis et utile.`;

      const geminiChatHistory = [];
      
      conversationHistory[senderID].slice(-5).forEach(exchange => {
          geminiChatHistory.push({ role: "user", parts: exchange.userParts });
          geminiChatHistory.push({ role: "model", parts: [{ text: exchange.aiResponse }] });
      });
      
      geminiChatHistory.push({ role: "user", parts: geminiParts });

      const apiUrl = getGeminiApiUrl(GEMINI_FLASH_MODEL, API_KEY);
      
      const payload = {
          contents: geminiChatHistory,
          systemInstruction: { parts: [{ text: systemPrompt }] }
      };
      
      const response = await axios.post(apiUrl, payload, { headers: { 'Content-Type': 'application/json' } });
      
      if (waitingMessageID) api.unsendMessage(waitingMessageID);

      let answer = response.data?.candidates?.[0]?.content?.parts?.[0]?.text;
      
      if (!answer) answer = "‚ö†Ô∏è R√©ponse vide de l'IA (S√©curit√© ou erreur interne).";
      
      conversationHistory[senderID].push({
          userParts: geminiParts, 
          aiResponse: answer,
          timestamp: Date.now()
      });
      
      if (conversationHistory[senderID].length > 10) conversationHistory[senderID].shift(); 

      const responseTitle = imageAttachment ? "ü§ñ ùóöùó£ùóß-ùü± ùó©ùó∂ùòÄùó∂ùóºùóª" : "ü§ñ ùóöùó£ùóß-ùü±";

      const finalAnswer = `‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n ${responseTitle}\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n${answer}\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ`; 

      api.sendMessage(finalAnswer, threadID, (err, info) => {
          if (!err && info) botMessageIDs.add(info.messageID);
          api.setMessageReaction('‚úÖ', userMessageID, (err) => {}, true);
      });
      
    } catch (error) {
        console.error("Erreur GPT5:", error.message);
        if (waitingMessageID) api.unsendMessage(waitingMessageID);
        api.sendMessage("‚ùå Erreur de connexion √† Gemini API. V√©rifiez les logs Render.", event.threadID);
        api.setMessageReaction('‚ùå', userMessageID, (err) => {}, true);
    }
  }
};
